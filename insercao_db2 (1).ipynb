{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "### Gerar token do projeto"}, {"metadata": {}, "cell_type": "code", "source": "from project_lib import Project\nproject = Project(spark.sparkContext, '############################', '#########################')\npc = project.project_context", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Importar bibliotecas"}, {"metadata": {}, "cell_type": "code", "source": "import sys\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\nimport ibmos2spark, os\nfrom pyspark.sql import SparkSession\nimport os, ibm_db_dbi as dbi\nimport ibm_db", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Creadenciais DB2"}, {"metadata": {}, "cell_type": "code", "source": "DB2_Hackathon_cert = os.path.join(os.path.expanduser('~'),'DB2_Hackathon_ssl.cert')\nf = open(DB2_Hackathon_cert, \"w\")\nf.write(\"\"\"#######################\"\"\")\nf.close()\n\nDB2_Hackathon_dsn = 'DATABASE={};HOSTNAME={};PORT={};PROTOCOL=TCPIP;UID={uid};PWD={pwd};SECURITY=SSL;'.format(\n    '############################################',\n    '############################################',\n    #####,\n    uid='################',\n    pwd=\"\"\"################\"\"\",\n    cert=DB2_Hackathon_cert\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Credenciais CSV"}, {"metadata": {}, "cell_type": "code", "source": "import ibmos2spark, os\n# @hidden_cell\n\nif os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n    endpoint_1 = '########################################################'\nelse:\n    endpoint_1 = '########################################################'\n\ncredentials = {\n    'endpoint': endpoint_1,\n    'service_id': '##########################################',\n    'iam_service_endpoint': '############################################',\n    'api_key': '########################################'\n}\n\nconfiguration_name = '############################################'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Tratamento DataFrame e Carregando no DB2"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nscore_v0 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('score_v0.csv', '#############################################'))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# pd.options.display.max_rows = 1000000", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Converter DataFrame Spark para Pandas"}, {"metadata": {}, "cell_type": "code", "source": "df = score_v0.toPandas()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Variavel de conex\u00e3o com o server DB2"}, {"metadata": {}, "cell_type": "code", "source": "con = ibm_db.connect(DB2_Hackathon_dsn,\"\",\"\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "N\u00famero de colunas"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "len(df.columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Nome colunas"}, {"metadata": {}, "cell_type": "code", "source": "df.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Query para inser\u00e7\u00e3o dos dados"}, {"metadata": {}, "cell_type": "code", "source": "sql = \"INSERT INTO ################### VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Prepara\u00e7\u00e3o da query e variavel de conex\u00e3o"}, {"metadata": {}, "cell_type": "code", "source": "stmt = ibm_db.prepare(con, sql)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Gerando novo DataFrame"}, {"metadata": {}, "cell_type": "code", "source": "subset = df[[list_columns]]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Convertendo os registros para o tipo tuple"}, {"metadata": {}, "cell_type": "code", "source": "tuple_of_tuples = tuple([tuple(x) for x in subset.values])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Executando a query com os dados da variavel \"tuple_of_tuples\""}, {"metadata": {}, "cell_type": "code", "source": "ibm_db.execute_many(stmt, tuple_of_tuples)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python37", "display_name": "Python 3.7 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.7.10", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}